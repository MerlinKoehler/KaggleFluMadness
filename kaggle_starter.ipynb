{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEN3450, Data Analysis 2020 \n",
    "\n",
    "**Kaggle Competition 2020**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import graphviz\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "#import your classifiers here\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 500\n",
    "# References https://stackoverflow.com/questions/46543060/how-to-replace-every-nan-in-a-column-with-different-random-values-using-pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosing the Maastricht Flu \n",
    "\n",
    "You are given the early data for an outbreak of a dangerous virus originating from a group of primates being kept in a Maastricht biomedical research lab in the basement of Henri-Paul Spaaklaan building, this virus is dubbed the \"Maastricht Flu\".\n",
    "\n",
    "You have the medical records of $n$ number of patients in `flu_train.csv`. There are two general types of patients in the data, flu patients and healthy (this is recorded in the column labeled `flu`, a 0 indicates the absences of the virus and a 1 indicates presence). Notice that the dataset is unbalanced and you can expect a similar imbalance in the testing set.\n",
    "\n",
    "**Your task:** build a model to predict if a given patient has the flu. Your goal is to catch as many flu patients as possible without misdiagnosing too many healthy patients.\n",
    "\n",
    "**The deliverable:** submit your final solution via Kaggle competition using the `flu_test.csv` data.\n",
    "\n",
    "Maastricht Gemeente will use your model to diagnose sets of future patients (held by us). You can expect that there will be an increase in the number of flu patients in any groups of patients in the future.\n",
    "\n",
    "Here are some benchmarks for comparison and for expectation management. Notice that because the dataset is unbalanced, we expect that there is going to be a large difference in the accuracy for each class, thus `accuracy` is a metric that might be misleading in this case (see also below). That's why the baselines below are based on the expected accuracy **per class** and also they give you an estimate for the AUROC on all patients in the testing data. This is the score you see in the Kaggle submission as well.\n",
    "\n",
    "**Baseline Model:** \n",
    "- ~50% expected accuracy on healthy patients in training data\n",
    "- ~50% expected accuracy on flu patients in training data\n",
    "- ~50% expected accuracy on healthy patients in testing data (future data, no info on the labels)\n",
    "- ~50% expected accuracy on flu patients in testing data (future data, no info on the labels)\n",
    "- ~50% expected AUROC on all patients in testing data (future data, no info on the labels)\n",
    "\n",
    "**Reasonable Model:** \n",
    "- ~70% expected accuracy on healthy patients in training data\n",
    "- ~55% expected accuracy on flu patients, in training data\n",
    "- ~70% expected accuracy on healthy patients in testing data (future data, no info on the labels, to be checked upon your submission)\n",
    "- ~57% expected accuracy on flu patients, in testing data (future data, no info on the labels, to be checked upon your submission)\n",
    "- ~65% expected AUROC on all patients, in testing data (future data, no info on the labels, to be checked from Kaggle)\n",
    "\n",
    "**Grading:**\n",
    "Your grade will be based on:\n",
    "1. your model's ability to out-perform the benchmarks (they are kind of low, so we won't care much about this)\n",
    "2. your ability to carefully and thoroughly follow the data analysis pipeline\n",
    "3. the extend to which all choices are reasonable and defensible by methods you have learned in this class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read the data, clean and explore the data\n",
    "\n",
    "There are a large number of missing values in the data. Nearly all predictors have some degree of missingness. Not all missingness are alike: NaN in the `'pregnancy'` column is meaningful and informative, as patients with NaN's in the pregnancy column are males, where as NaN's in other predictors may appear randomly. \n",
    "\n",
    "\n",
    "**What do you do?:** We make no attempt to interpret the predictors and we make no attempt to model the missing values in the data in any meaningful way. We replace all missing values with 0.\n",
    "\n",
    "However, it would be more complete to look at the data and allow the data to inform your decision on how to address missingness. For columns where NaN values are informative, you might want to treat NaN as a distinct value; You might want to drop predictors with too many missing values and impute the ones with few missing values using a model. There are many acceptable strategies here, as long as the appropriateness of the method in the context of the task and the data is discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "def fillNaN_with_unifrand(df):\n",
    "    a = df.values\n",
    "    m = np.isnan(a) # mask of NaNs\n",
    "    mu, sigma = df.mean(), df.std()\n",
    "    a[m] = np.random.normal(mu, sigma, size=m.sum())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_train(df):\n",
    "    df_cleaned_train = df.copy()\n",
    "\n",
    "    # Changed Male/Female to 1/0 for a boolean datatype\n",
    "    df_cleaned_train.loc[(df_cleaned_train['Gender'] == 'male'), 'Gender'] = 1\n",
    "    df_cleaned_train.loc[(df_cleaned_train['Gender'] == 'female'), 'Gender'] = 0\n",
    "    df_cleaned_train.dropna(subset=['Gender'], inplace=True)\n",
    "\n",
    "    # Age of 0 is okay, we assume this are babies.\n",
    "    #display(df_cleaned_train[df_cleaned_train['Age'] == 0]['Weight'].describe())\n",
    "\n",
    "\n",
    "    # Set everybody under age of 14 to No Degree\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Education'])) & (df_cleaned_train['Age'] < 14), 'Education'] = 'No Degree'\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Education'])), 'Education'] = 'Unknown'\n",
    "\n",
    "    # Set Marital Status to unknown for the missing values\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['MaritalStatus'])), 'MaritalStatus'] = 'Unknown'\n",
    "\n",
    "    # Delete all rows where HHIncome is nan\n",
    "    df_cleaned_train.dropna(subset=['HHIncomeMid'], inplace=True)\n",
    "\n",
    "    #df_cleaned_train[pd.isna(df_cleaned_train['Poverty'])]\n",
    "    #df_cleaned_train.plot.scatter(y = 'Poverty', x='HHIncomeMid')\n",
    "    # Deleting missing poverty values for now, if it is import we can do a lin reg later on.\n",
    "    df_cleaned_train.loc[(df_cleaned_train['Poverty'] == 0) & (df_cleaned_train['HHIncome'] == 'more 99999'), 'Poverty'] = math.nan\n",
    "    df_cleaned_train.dropna(subset=['Poverty'], inplace=True)\n",
    "\n",
    "    del df_cleaned_train['HHIncome']\n",
    "    #display(df_cleaned_train[df_cleaned_train['Poverty'] == 0])\n",
    "\n",
    "\n",
    "    df_cleaned_train.dropna(subset=['HomeRooms'], inplace=True)\n",
    "    df_cleaned_train.dropna(subset=['HomeOwn'], inplace=True)\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Work'])), 'Work'] = 'Unknown'\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Weight'])) & (df_cleaned_train.flu == 1), 'Weight'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['Weight'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Weight'])) & (df_cleaned_train.flu == 0), 'Weight'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['Weight'])\n",
    "\n",
    "\n",
    "    # Height and Length describe the same values, so we copy it from one to the other.\n",
    "    df_cleaned_train.loc[((pd.notna(df_cleaned_train.Length)) & (pd.isna(df_cleaned_train.Height))), 'Height'] = df_cleaned_train[(pd.notna(df_cleaned_train.Length)) & (pd.isna(df_cleaned_train.Height))][['Length']]\n",
    "    df_cleaned_train.dropna(subset=['Height'], inplace=True)\n",
    "    del df_cleaned_train['Length']\n",
    "\n",
    "    del df_cleaned_train['HeadCirc']\n",
    "    del df_cleaned_train['BMICatUnder20yrs']\n",
    "    del df_cleaned_train['BMI_WHO']\n",
    "    df_cleaned_train.dropna(subset=['BMI'], inplace=True)\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Pulse'])) & (df_cleaned_train.flu == 1), 'Pulse'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['Pulse'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Pulse'])) & (df_cleaned_train.flu == 0), 'Pulse'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['Pulse'])\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPSysAve'])) & (df_cleaned_train.flu == 1), 'BPSysAve'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['BPSysAve'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPSysAve'])) & (df_cleaned_train.flu == 0), 'BPSysAve'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['BPSysAve'])\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPDiaAve'])) & (df_cleaned_train.flu == 1), 'BPDiaAve'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['BPDiaAve'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPDiaAve'])) & (df_cleaned_train.flu == 0), 'BPDiaAve'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['BPDiaAve'])\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPSys1'])) & (df_cleaned_train.flu == 1), 'BPSys1'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['BPSys1'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPSys1'])) & (df_cleaned_train.flu == 0), 'BPSys1'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['BPSys1'])\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPDia1'])) & (df_cleaned_train.flu == 1), 'BPDia1'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['BPDia1'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPDia1'])) & (df_cleaned_train.flu == 0), 'BPDia1'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['BPDia1'])\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPSys2'])) & (df_cleaned_train.flu == 1), 'BPSys2'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['BPSys2'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPSys2'])) & (df_cleaned_train.flu == 0), 'BPSys2'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['BPSys2'])\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPDia2'])) & (df_cleaned_train.flu == 1), 'BPDia2'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['BPDia2'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPDia2'])) & (df_cleaned_train.flu == 0), 'BPDia2'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['BPDia2'])\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPSys3'])) & (df_cleaned_train.flu == 1), 'BPSys3'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['BPSys3'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPSys3'])) & (df_cleaned_train.flu == 0), 'BPSys3'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['BPSys3'])\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPDia3'])) & (df_cleaned_train.flu == 1), 'BPDia3'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['BPDia3'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['BPDia3'])) & (df_cleaned_train.flu == 0), 'BPDia3'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['BPDia3'])\n",
    "\n",
    "    df_cleaned_train.Testosterone = df_cleaned_train.Testosterone.astype(float)\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Testosterone'])) & (df_cleaned_train.flu == 1), 'Testosterone'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['Testosterone'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Testosterone'])) & (df_cleaned_train.flu == 0), 'Testosterone'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['Testosterone'])\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['DirectChol'])) & (df_cleaned_train.flu == 1), 'DirectChol'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['DirectChol'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['DirectChol'])) & (df_cleaned_train.flu == 0), 'DirectChol'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['DirectChol'])\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['TotChol'])) & (df_cleaned_train.flu == 1), 'TotChol'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['TotChol'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['TotChol'])) & (df_cleaned_train.flu == 0), 'TotChol'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['TotChol'])\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['UrineVol1'])) & (df_cleaned_train.flu == 1), 'UrineVol1'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['UrineVol1'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['UrineVol1'])) & (df_cleaned_train.flu == 0), 'UrineVol1'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['UrineVol1'])\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['UrineFlow1'])) & (df_cleaned_train.flu == 1), 'UrineFlow1'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['UrineFlow1'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['UrineFlow1'])) & (df_cleaned_train.flu == 0), 'UrineFlow1'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['UrineFlow1'])\n",
    "\n",
    "    del df_cleaned_train['UrineVol2']\n",
    "    del df_cleaned_train['UrineFlow2']\n",
    "\n",
    "    df_cleaned_train.loc[(df_cleaned_train['Diabetes'] == 'Yes'), 'Diabetes'] = 1\n",
    "    df_cleaned_train.loc[(df_cleaned_train['Diabetes'] == 'No'), 'Diabetes'] = 0\n",
    "    df_cleaned_train.dropna(subset=['Diabetes'], inplace=True)\n",
    "    del df_cleaned_train['DiabetesAge']\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['HealthGen'])), 'HealthGen'] = 'Unknown'\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['DaysMentHlthBad'])) & (df_cleaned_train.flu == 1), 'DaysMentHlthBad'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['DaysMentHlthBad'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['DaysMentHlthBad'])) & (df_cleaned_train.flu == 0), 'DaysMentHlthBad'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['DaysMentHlthBad'])\n",
    "\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['LittleInterest'])), 'LittleInterest'] = 'Unknown'\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Depressed'])), 'Depressed'] = 'Unknown'\n",
    "\n",
    "    # Missing values in \"nPregnancies\", \"nBabies\", \"Age1stBaby\" to 0, it isn't logical to take the average here.\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['nPregnancies'])), 'nPregnancies'] = 0\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['nBabies'])), 'nBabies'] = 0\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Age1stBaby'])), 'Age1stBaby'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['SleepHrsNight'])) & (df_cleaned_train.flu == 1), 'SleepHrsNight'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['SleepHrsNight'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['SleepHrsNight'])) & (df_cleaned_train.flu == 0), 'SleepHrsNight'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['SleepHrsNight'])\n",
    "\n",
    "    df_cleaned_train.loc[(df_cleaned_train['SleepTrouble'] == 'Yes'), 'SleepTrouble'] = 1\n",
    "    df_cleaned_train.loc[(df_cleaned_train['SleepTrouble'] == 'No'), 'SleepTrouble'] = 0\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['SleepTrouble'])), 'SleepTrouble'] = 0\n",
    "    #df_cleaned_train.loc[(pd.isnull(df_cleaned_train['SleepTrouble'])), 'SleepTrouble'] = 'Unknown'\n",
    "\n",
    "    df_cleaned_train.loc[(df_cleaned_train['PhysActive'] == 'Yes'), 'PhysActive'] = 1\n",
    "    df_cleaned_train.loc[(df_cleaned_train['PhysActive'] == 'No'), 'PhysActive'] = 0\n",
    "    # Maybe change this later on\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['PhysActive'])), 'PhysActive'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['PhysActiveDays'])), 'PhysActiveDays'] = 0\n",
    "\n",
    "    for index, row in df_cleaned_train.iterrows():\n",
    "        df_cleaned_train.loc[index,'TVHrsDay'] = str(row['TVHrsDay']).replace(\"More_4_hr\", \"5\")\n",
    "        df_cleaned_train.loc[index,'CompHrsDay'] = str(row['CompHrsDay']).replace(\"More_4_hr\", \"5\")\n",
    "    for index, row in df_cleaned_train.iterrows():\n",
    "        df_cleaned_train.loc[index,'TVHrsDay'] = re.sub('[A-Za-z_]', '', str(row['TVHrsDay']))\n",
    "        df_cleaned_train.loc[index,'CompHrsDay'] = re.sub('[A-Za-z_]', '', str(row['CompHrsDay']))\n",
    "\n",
    "    df_cleaned_train['TVHrsDay'] = pd.to_numeric(df_cleaned_train['TVHrsDay'], errors='coerce')\n",
    "    df_cleaned_train['CompHrsDay'] = pd.to_numeric(df_cleaned_train['CompHrsDay'], errors='coerce')\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['TVHrsDay'])) & (df_cleaned_train.flu == 1), 'TVHrsDay'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['TVHrsDay'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['TVHrsDay'])) & (df_cleaned_train.flu == 0), 'TVHrsDay'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['TVHrsDay'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['CompHrsDay'])) & (df_cleaned_train.flu == 1), 'CompHrsDay'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 1]['CompHrsDay'])\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['CompHrsDay'])) & (df_cleaned_train.flu == 0), 'CompHrsDay'] = fillNaN_with_unifrand(df_cleaned_train[df_cleaned_train.flu == 0]['CompHrsDay'])\n",
    "\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['TVHrsDay'])), 'TVHrsDay'] = 'Unknown'\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['CompHrsDay'])), 'CompHrsDay'] = 'Unknown'\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['TVHrsDayChild'])), 'TVHrsDayChild'] = 0\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['CompHrsDayChild'])), 'CompHrsDayChild'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(df_cleaned_train['Alcohol12PlusYr'] == 'Yes'), 'Alcohol12PlusYr'] = 1\n",
    "    df_cleaned_train.loc[(df_cleaned_train['Alcohol12PlusYr'] == 'No'), 'Alcohol12PlusYr'] = 0\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Alcohol12PlusYr'])), 'Alcohol12PlusYr'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['AlcoholDay'])), 'AlcoholDay'] = 0\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['AlcoholYear'])), 'AlcoholYear'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(df_cleaned_train['SmokeNow'] == 'Yes'), 'SmokeNow'] = 1\n",
    "    df_cleaned_train.loc[(df_cleaned_train['SmokeNow'] == 'No'), 'SmokeNow'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(df_cleaned_train['Smoke100'] == 'Yes'), 'Smoke100'] = 1\n",
    "    df_cleaned_train.loc[(df_cleaned_train['Smoke100'] == 'No'), 'Smoke100'] = 0\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Smoke100'])), 'Smoke100'] = 0\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Smoke100n'])) & (df_cleaned_train['Smoke100'] == 1), 'Smoke100n'] = 'Smoker'\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Smoke100n'])), 'Smoke100n'] = 'Unknown'\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['SmokeNow'])) & (df_cleaned_train['Smoke100n'] == 'Smoker'), 'SmokeNow'] = 1\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['SmokeNow'])), 'SmokeNow'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['SmokeAge'])), 'SmokeAge'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(df_cleaned_train['Marijuana'] == 'Yes'), 'Marijuana'] = 1\n",
    "    df_cleaned_train.loc[(df_cleaned_train['Marijuana'] == 'No'), 'Marijuana'] = 0\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['Marijuana'])), 'Marijuana'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['AgeFirstMarij'])), 'AgeFirstMarij'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(df_cleaned_train['RegularMarij'] == 'Yes'), 'RegularMarij'] = 1\n",
    "    df_cleaned_train.loc[(df_cleaned_train['RegularMarij'] == 'No'), 'RegularMarij'] = 0\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['RegularMarij'])), 'RegularMarij'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['AgeRegMarij'])), 'AgeRegMarij'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(df_cleaned_train['HardDrugs'] == 'Yes'), 'HardDrugs'] = 1\n",
    "    df_cleaned_train.loc[(df_cleaned_train['HardDrugs'] == 'No'), 'HardDrugs'] = 0\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['HardDrugs'])), 'HardDrugs'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(df_cleaned_train['SexEver'] == 'Yes'), 'SexEver'] = 1\n",
    "    df_cleaned_train.loc[(df_cleaned_train['SexEver'] == 'No'), 'SexEver'] = 0\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['SexEver'])), 'SexEver'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['SexAge'])), 'SexAge'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['SexNumPartnLife'])), 'SexNumPartnLife'] = 0\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['SexNumPartYear'])), 'SexNumPartYear'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(df_cleaned_train['SameSex'] == 'Yes'), 'SameSex'] = 1\n",
    "    df_cleaned_train.loc[(df_cleaned_train['SameSex'] == 'No'), 'SameSex'] = 0\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['SameSex'])), 'SameSex'] = 0\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['SexOrientation'])), 'SexOrientation'] = 'Unknown'\n",
    "\n",
    "    df_cleaned_train.loc[(pd.isna(df_cleaned_train['PregnantNow'])), 'PregnantNow'] = 0\n",
    "    df_cleaned_train.loc[df_cleaned_train['PregnantNow'] == 'Unknown', 'PregnantNow'] = 0\n",
    "    df_cleaned_train.loc[(df_cleaned_train['PregnantNow'] == 'Yes'), 'PregnantNow'] = 1\n",
    "    df_cleaned_train.loc[(df_cleaned_train['PregnantNow'] == 'No'), 'PregnantNow'] = 0\n",
    "\n",
    "    df_cleaned_train['Gender'] = pd.to_numeric(df_cleaned_train['Gender'])\n",
    "    df_cleaned_train['Diabetes'] = pd.to_numeric(df_cleaned_train['Diabetes'])\n",
    "    df_cleaned_train['SleepTrouble'] = pd.to_numeric(df_cleaned_train['SleepTrouble'])\n",
    "    df_cleaned_train['PhysActive'] = pd.to_numeric(df_cleaned_train['PhysActive'])\n",
    "    df_cleaned_train['Alcohol12PlusYr'] = pd.to_numeric(df_cleaned_train['Alcohol12PlusYr'])\n",
    "    df_cleaned_train['SmokeNow'] = pd.to_numeric(df_cleaned_train['SmokeNow'])\n",
    "    df_cleaned_train['Smoke100'] = pd.to_numeric(df_cleaned_train['Smoke100'])\n",
    "    df_cleaned_train['Marijuana'] = pd.to_numeric(df_cleaned_train['Marijuana'])\n",
    "    df_cleaned_train['RegularMarij'] = pd.to_numeric(df_cleaned_train['RegularMarij'])\n",
    "    df_cleaned_train['HardDrugs'] = pd.to_numeric(df_cleaned_train['HardDrugs'])\n",
    "    df_cleaned_train['SexEver'] = pd.to_numeric(df_cleaned_train['SexEver'])\n",
    "    df_cleaned_train['SameSex'] = pd.to_numeric(df_cleaned_train['SameSex'])\n",
    "    df_cleaned_train['PregnantNow'] = pd.to_numeric(df_cleaned_train['PregnantNow'])\n",
    "    df_cleaned_train['TVHrsDay'] = pd.to_numeric(df_cleaned_train['TVHrsDay'])\n",
    "    df_cleaned_train['CompHrsDay'] = pd.to_numeric(df_cleaned_train['CompHrsDay'])\n",
    "    \n",
    "    df_cleaned_train = df_cleaned_train.append(pd.get_dummies(df_cleaned_train[['Race1', 'Education', 'MaritalStatus', 'HomeOwn', 'Work', 'HealthGen', 'LittleInterest', 'Depressed', 'TVHrsDay', 'CompHrsDay', 'Smoke100n', 'SexOrientation', 'PregnantNow']]))\n",
    "    list_del = ['Race1', 'Education', 'MaritalStatus', 'HomeOwn', 'Work', 'HealthGen', 'LittleInterest', 'Depressed', 'TVHrsDay', 'CompHrsDay', 'Smoke100n', 'SexOrientation', 'PregnantNow']\n",
    "    for itm in list_del:\n",
    "        del df_cleaned_train[itm]\n",
    "    \n",
    "    df_cleaned_train.fillna(0, inplace=True)\n",
    "    \n",
    "    return df_cleaned_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_test(df):\n",
    "    df_cleaned_test = df.copy()\n",
    "\n",
    "    # Changed Male/Female to 1/0 for a boolean datatype\n",
    "    df_cleaned_test.loc[(df_cleaned_test['Gender'] == 'male'), 'Gender'] = 1\n",
    "    df_cleaned_test.loc[(df_cleaned_test['Gender'] == 'female'), 'Gender'] = 0\n",
    "    df_cleaned_test.dropna(subset=['Gender'], inplace=True)\n",
    "\n",
    "    # Age of 0 is okay, we assume this are babies.\n",
    "    #display(df_cleaned_test[df_cleaned_test['Age'] == 0]['Weight'].describe())\n",
    "\n",
    "\n",
    "    # Set everybody under age of 14 to No Degree\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['Education'])) & (df_cleaned_test['Age'] < 14), 'Education'] = 'No Degree'\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['Education'])), 'Education'] = 'Unknown'\n",
    "\n",
    "    # Set Marital Status to unknown for the missing values\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['MaritalStatus'])), 'MaritalStatus'] = 'Unknown'\n",
    "\n",
    "    # Delete all rows where HHIncome is nan\n",
    "    df_cleaned_test.dropna(subset=['HHIncomeMid'], inplace=True)\n",
    "\n",
    "    #df_cleaned_test[pd.isna(df_cleaned_test['Poverty'])]\n",
    "    #df_cleaned_test.plot.scatter(y = 'Poverty', x='HHIncomeMid')\n",
    "    # Deleting missing poverty values for now, if it is import we can do a lin reg later on.\n",
    "    df_cleaned_test.loc[(df_cleaned_test['Poverty'] == 0) & (df_cleaned_test['HHIncome'] == 'more 99999'), 'Poverty'] = math.nan\n",
    "    df_cleaned_test.dropna(subset=['Poverty'], inplace=True)\n",
    "\n",
    "    del df_cleaned_test['HHIncome']\n",
    "    #display(df_cleaned_test[df_cleaned_test['Poverty'] == 0])\n",
    "\n",
    "\n",
    "    df_cleaned_test.dropna(subset=['HomeRooms'], inplace=True)\n",
    "    df_cleaned_test.dropna(subset=['HomeOwn'], inplace=True)\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['Work'])), 'Work'] = 'Unknown'\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['Weight'])), 'Weight'] = fillNaN_with_unifrand(df_cleaned_test['Weight'])\n",
    "\n",
    "\n",
    "    # Height and Length describe the same values, so we copy it from one to the other.\n",
    "    df_cleaned_test.loc[((pd.notna(df_cleaned_test.Length)) & (pd.isna(df_cleaned_test.Height))), 'Height'] = df_cleaned_test[(pd.notna(df_cleaned_test.Length)) & (pd.isna(df_cleaned_test.Height))][['Length']]\n",
    "    df_cleaned_test.dropna(subset=['Height'], inplace=True)\n",
    "    del df_cleaned_test['Length']\n",
    "\n",
    "    del df_cleaned_test['HeadCirc']\n",
    "    del df_cleaned_test['BMICatUnder20yrs']\n",
    "    del df_cleaned_test['BMI_WHO']\n",
    "    df_cleaned_test.dropna(subset=['BMI'], inplace=True)\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['Pulse'])), 'Pulse'] = fillNaN_with_unifrand(df_cleaned_test['Pulse'])\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['BPSysAve'])), 'BPSysAve'] = fillNaN_with_unifrand(df_cleaned_test['BPSysAve'])\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['BPDiaAve'])), 'BPDiaAve'] = fillNaN_with_unifrand(df_cleaned_test['BPDiaAve'])\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['BPSys1'])), 'BPSys1'] = fillNaN_with_unifrand(df_cleaned_test['BPSys1'])\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['BPDia1'])), 'BPDia1'] = fillNaN_with_unifrand(df_cleaned_test['BPDia1'])\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['BPSys2'])), 'BPSys2'] = fillNaN_with_unifrand(df_cleaned_test['BPSys2'])\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['BPDia2'])), 'BPDia2'] = fillNaN_with_unifrand(df_cleaned_test['BPDia2'])\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['BPSys3'])), 'BPSys3'] = fillNaN_with_unifrand(df_cleaned_test['BPSys3'])\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['BPDia3'])), 'BPDia3'] = fillNaN_with_unifrand(df_cleaned_test['BPDia3'])\n",
    "\n",
    "    df_cleaned_test.Testosterone = df_cleaned_test.Testosterone.astype(float)\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['Testosterone'])), 'Testosterone'] = fillNaN_with_unifrand(df_cleaned_test['Testosterone'])\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['DirectChol'])), 'DirectChol'] = fillNaN_with_unifrand(df_cleaned_test['DirectChol'])\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['TotChol'])), 'TotChol'] = fillNaN_with_unifrand(df_cleaned_test['TotChol'])\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['UrineVol1'])), 'UrineVol1'] = fillNaN_with_unifrand(df_cleaned_test['UrineVol1'])\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['UrineFlow1'])), 'UrineFlow1'] = fillNaN_with_unifrand(df_cleaned_test['UrineFlow1'])\n",
    "\n",
    "    del df_cleaned_test['UrineVol2']\n",
    "    del df_cleaned_test['UrineFlow2']\n",
    "\n",
    "    df_cleaned_test.loc[(df_cleaned_test['Diabetes'] == 'Yes'), 'Diabetes'] = 1\n",
    "    df_cleaned_test.loc[(df_cleaned_test['Diabetes'] == 'No'), 'Diabetes'] = 0\n",
    "    df_cleaned_test.dropna(subset=['Diabetes'], inplace=True)\n",
    "    del df_cleaned_test['DiabetesAge']\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['HealthGen'])), 'HealthGen'] = 'Unknown'\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['DaysMentHlthBad'])), 'DaysMentHlthBad'] = fillNaN_with_unifrand(df_cleaned_test['DaysMentHlthBad'])\n",
    "\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['LittleInterest'])), 'LittleInterest'] = 'Unknown'\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['Depressed'])), 'Depressed'] = 'Unknown'\n",
    "\n",
    "    # Missing values in \"nPregnancies\", \"nBabies\", \"Age1stBaby\" to 0, it isn't logical to take the average here.\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['nPregnancies'])), 'nPregnancies'] = 0\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['nBabies'])), 'nBabies'] = 0\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['Age1stBaby'])), 'Age1stBaby'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['SleepHrsNight'])), 'SleepHrsNight'] = fillNaN_with_unifrand(df_cleaned_test['SleepHrsNight'])\n",
    "\n",
    "    df_cleaned_test.loc[(df_cleaned_test['SleepTrouble'] == 'Yes'), 'SleepTrouble'] = 1\n",
    "    df_cleaned_test.loc[(df_cleaned_test['SleepTrouble'] == 'No'), 'SleepTrouble'] = 0\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['SleepTrouble'])), 'SleepTrouble'] = 0\n",
    "    #df_cleaned_test.loc[(pd.isnull(df_cleaned_test['SleepTrouble'])), 'SleepTrouble'] = 'Unknown'\n",
    "\n",
    "    df_cleaned_test.loc[(df_cleaned_test['PhysActive'] == 'Yes'), 'PhysActive'] = 1\n",
    "    df_cleaned_test.loc[(df_cleaned_test['PhysActive'] == 'No'), 'PhysActive'] = 0\n",
    "    # Maybe change this later on\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['PhysActive'])), 'PhysActive'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['PhysActiveDays'])), 'PhysActiveDays'] = 0\n",
    "\n",
    "    for index, row in df_cleaned_test.iterrows():\n",
    "        df_cleaned_test.loc[index,'TVHrsDay'] = str(row['TVHrsDay']).replace(\"More_4_hr\", \"5\")\n",
    "        df_cleaned_test.loc[index,'CompHrsDay'] = str(row['CompHrsDay']).replace(\"More_4_hr\", \"5\")\n",
    "    for index, row in df_cleaned_test.iterrows():\n",
    "        df_cleaned_test.loc[index,'TVHrsDay'] = re.sub('[A-Za-z_]', '', str(row['TVHrsDay']))\n",
    "        df_cleaned_test.loc[index,'CompHrsDay'] = re.sub('[A-Za-z_]', '', str(row['CompHrsDay']))\n",
    "\n",
    "    df_cleaned_test['TVHrsDay'] = pd.to_numeric(df_cleaned_test['TVHrsDay'], errors='coerce')\n",
    "    df_cleaned_test['CompHrsDay'] = pd.to_numeric(df_cleaned_test['CompHrsDay'], errors='coerce')\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['TVHrsDay'])), 'TVHrsDay'] = fillNaN_with_unifrand(df_cleaned_test['TVHrsDay'])\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['CompHrsDay'])), 'CompHrsDay'] = fillNaN_with_unifrand(df_cleaned_test['CompHrsDay'])\n",
    "\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['TVHrsDay'])), 'TVHrsDay'] = 'Unknown'\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['CompHrsDay'])), 'CompHrsDay'] = 'Unknown'\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['TVHrsDayChild'])), 'TVHrsDayChild'] = 0\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['CompHrsDayChild'])), 'CompHrsDayChild'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(df_cleaned_test['Alcohol12PlusYr'] == 'Yes'), 'Alcohol12PlusYr'] = 1\n",
    "    df_cleaned_test.loc[(df_cleaned_test['Alcohol12PlusYr'] == 'No'), 'Alcohol12PlusYr'] = 0\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['Alcohol12PlusYr'])), 'Alcohol12PlusYr'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['AlcoholDay'])), 'AlcoholDay'] = 0\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['AlcoholYear'])), 'AlcoholYear'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(df_cleaned_test['SmokeNow'] == 'Yes'), 'SmokeNow'] = 1\n",
    "    df_cleaned_test.loc[(df_cleaned_test['SmokeNow'] == 'No'), 'SmokeNow'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(df_cleaned_test['Smoke100'] == 'Yes'), 'Smoke100'] = 1\n",
    "    df_cleaned_test.loc[(df_cleaned_test['Smoke100'] == 'No'), 'Smoke100'] = 0\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['Smoke100'])), 'Smoke100'] = 0\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['Smoke100n'])) & (df_cleaned_test['Smoke100'] == 1), 'Smoke100n'] = 'Smoker'\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['Smoke100n'])), 'Smoke100n'] = 'Unknown'\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['SmokeNow'])) & (df_cleaned_test['Smoke100n'] == 'Smoker'), 'SmokeNow'] = 1\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['SmokeNow'])), 'SmokeNow'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['SmokeAge'])), 'SmokeAge'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(df_cleaned_test['Marijuana'] == 'Yes'), 'Marijuana'] = 1\n",
    "    df_cleaned_test.loc[(df_cleaned_test['Marijuana'] == 'No'), 'Marijuana'] = 0\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['Marijuana'])), 'Marijuana'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['AgeFirstMarij'])), 'AgeFirstMarij'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(df_cleaned_test['RegularMarij'] == 'Yes'), 'RegularMarij'] = 1\n",
    "    df_cleaned_test.loc[(df_cleaned_test['RegularMarij'] == 'No'), 'RegularMarij'] = 0\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['RegularMarij'])), 'RegularMarij'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['AgeRegMarij'])), 'AgeRegMarij'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(df_cleaned_test['HardDrugs'] == 'Yes'), 'HardDrugs'] = 1\n",
    "    df_cleaned_test.loc[(df_cleaned_test['HardDrugs'] == 'No'), 'HardDrugs'] = 0\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['HardDrugs'])), 'HardDrugs'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(df_cleaned_test['SexEver'] == 'Yes'), 'SexEver'] = 1\n",
    "    df_cleaned_test.loc[(df_cleaned_test['SexEver'] == 'No'), 'SexEver'] = 0\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['SexEver'])), 'SexEver'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['SexAge'])), 'SexAge'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['SexNumPartnLife'])), 'SexNumPartnLife'] = 0\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['SexNumPartYear'])), 'SexNumPartYear'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(df_cleaned_test['SameSex'] == 'Yes'), 'SameSex'] = 1\n",
    "    df_cleaned_test.loc[(df_cleaned_test['SameSex'] == 'No'), 'SameSex'] = 0\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['SameSex'])), 'SameSex'] = 0\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['SexOrientation'])), 'SexOrientation'] = 'Unknown'\n",
    "\n",
    "    df_cleaned_test.loc[(pd.isna(df_cleaned_test['PregnantNow'])), 'PregnantNow'] = 0\n",
    "    df_cleaned_test.loc[df_cleaned_test['PregnantNow'] == 'Unknown', 'PregnantNow'] = 0\n",
    "    df_cleaned_test.loc[(df_cleaned_test['PregnantNow'] == 'Yes'), 'PregnantNow'] = 1\n",
    "    df_cleaned_test.loc[(df_cleaned_test['PregnantNow'] == 'No'), 'PregnantNow'] = 0\n",
    "\n",
    "    df_cleaned_test['Gender'] = pd.to_numeric(df_cleaned_test['Gender'])\n",
    "    df_cleaned_test['Diabetes'] = pd.to_numeric(df_cleaned_test['Diabetes'])\n",
    "    df_cleaned_test['SleepTrouble'] = pd.to_numeric(df_cleaned_test['SleepTrouble'])\n",
    "    df_cleaned_test['PhysActive'] = pd.to_numeric(df_cleaned_test['PhysActive'])\n",
    "    df_cleaned_test['Alcohol12PlusYr'] = pd.to_numeric(df_cleaned_test['Alcohol12PlusYr'])\n",
    "    df_cleaned_test['SmokeNow'] = pd.to_numeric(df_cleaned_test['SmokeNow'])\n",
    "    df_cleaned_test['Smoke100'] = pd.to_numeric(df_cleaned_test['Smoke100'])\n",
    "    df_cleaned_test['Marijuana'] = pd.to_numeric(df_cleaned_test['Marijuana'])\n",
    "    df_cleaned_test['RegularMarij'] = pd.to_numeric(df_cleaned_test['RegularMarij'])\n",
    "    df_cleaned_test['HardDrugs'] = pd.to_numeric(df_cleaned_test['HardDrugs'])\n",
    "    df_cleaned_test['SexEver'] = pd.to_numeric(df_cleaned_test['SexEver'])\n",
    "    df_cleaned_test['SameSex'] = pd.to_numeric(df_cleaned_test['SameSex'])\n",
    "    df_cleaned_test['PregnantNow'] = pd.to_numeric(df_cleaned_test['PregnantNow'])\n",
    "    df_cleaned_test['TVHrsDay'] = pd.to_numeric(df_cleaned_test['TVHrsDay'])\n",
    "    df_cleaned_test['CompHrsDay'] = pd.to_numeric(df_cleaned_test['CompHrsDay'])\n",
    "    \n",
    "    df_cleaned_test = df_cleaned_test.append(pd.get_dummies(df_cleaned_test[['Race1', 'Education', 'MaritalStatus', 'HomeOwn', 'Work', 'HealthGen', 'LittleInterest', 'Depressed', 'TVHrsDay', 'CompHrsDay', 'Smoke100n', 'SexOrientation', 'PregnantNow']]))\n",
    "    list_del = ['Race1', 'Education', 'MaritalStatus', 'HomeOwn', 'Work', 'HealthGen', 'LittleInterest', 'Depressed', 'TVHrsDay', 'CompHrsDay', 'Smoke100n', 'SexOrientation', 'PregnantNow']\n",
    "    for itm in list_del:\n",
    "        del df_cleaned_test[itm]\n",
    "    \n",
    "    df_cleaned_test.fillna(0, inplace=True)\n",
    "    \n",
    "    return df_cleaned_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "df = pd.read_csv('data/flu_train.csv')\n",
    "df = df[~np.isnan(df['flu'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "df_test = pd.read_csv('data/flu_test.csv')\n",
    "#df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (5246, 71)\n",
      "x test shape: (1533, 70)\n",
      "train class 0: 4936, train class 1: 310\n"
     ]
    }
   ],
   "source": [
    "#What's up in each set\n",
    "\n",
    "x = df.values[:, :-1]\n",
    "y = df.values[:, -1]\n",
    "\n",
    "x_test = df_test.values[:, :-1]\n",
    "\n",
    "print('x train shape:', x.shape)\n",
    "print('x test shape:', x_test.shape)\n",
    "print('train class 0: {}, train class 1: {}'.format(len(y[y==0]), len(y[y==1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(df, class_column, random_state=0):\n",
    "    # Better way with stratify:\n",
    "    itrain, itest = train_test_split(range(df.shape[0]), test_size=0.25, random_state=random_state, stratify=df[class_column])\n",
    "\n",
    "    columns = []\n",
    "    for column in df.columns:\n",
    "        if column != class_column:\n",
    "            columns.append(column)\n",
    "    gsstemp = df[columns]\n",
    "\n",
    "    X_train = gsstemp.iloc[itrain, :]\n",
    "    X_test = gsstemp.iloc[itest, :]\n",
    "    y_train = df[class_column].iloc[itrain]\n",
    "    y_test = df[class_column].iloc[itest]\n",
    "\n",
    "    print(\"Number of sick people in test set: \",len(y_test[y_test == 1]))\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sick people in test set:  78\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test(df, 'flu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['flu'] = y_train\n",
    "X_test['flu'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = clean_train(X_train)\n",
    "X_test = clean_test(X_test)\n",
    "\n",
    "y_train = X_train['flu']\n",
    "y_test = X_test['flu']\n",
    "del X_train['flu']\n",
    "del X_test['flu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6834 entries, 3498 to 1514\n",
      "Data columns (total 97 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   ID                           6834 non-null   float64\n",
      " 1   Gender                       6834 non-null   float64\n",
      " 2   Age                          6834 non-null   float64\n",
      " 3   HHIncomeMid                  6834 non-null   float64\n",
      " 4   Poverty                      6834 non-null   float64\n",
      " 5   HomeRooms                    6834 non-null   float64\n",
      " 6   Weight                       6834 non-null   float64\n",
      " 7   Height                       6834 non-null   float64\n",
      " 8   BMI                          6834 non-null   float64\n",
      " 9   Pulse                        6834 non-null   float64\n",
      " 10  BPSysAve                     6834 non-null   float64\n",
      " 11  BPDiaAve                     6834 non-null   float64\n",
      " 12  BPSys1                       6834 non-null   float64\n",
      " 13  BPDia1                       6834 non-null   float64\n",
      " 14  BPSys2                       6834 non-null   float64\n",
      " 15  BPDia2                       6834 non-null   float64\n",
      " 16  BPSys3                       6834 non-null   float64\n",
      " 17  BPDia3                       6834 non-null   float64\n",
      " 18  Testosterone                 6834 non-null   float64\n",
      " 19  DirectChol                   6834 non-null   float64\n",
      " 20  TotChol                      6834 non-null   float64\n",
      " 21  UrineVol1                    6834 non-null   float64\n",
      " 22  UrineFlow1                   6834 non-null   float64\n",
      " 23  Diabetes                     6834 non-null   float64\n",
      " 24  DaysMentHlthBad              6834 non-null   float64\n",
      " 25  nPregnancies                 6834 non-null   float64\n",
      " 26  nBabies                      6834 non-null   float64\n",
      " 27  Age1stBaby                   6834 non-null   float64\n",
      " 28  SleepHrsNight                6834 non-null   float64\n",
      " 29  SleepTrouble                 6834 non-null   float64\n",
      " 30  PhysActive                   6834 non-null   float64\n",
      " 31  PhysActiveDays               6834 non-null   float64\n",
      " 32  TVHrsDayChild                6834 non-null   float64\n",
      " 33  CompHrsDayChild              6834 non-null   float64\n",
      " 34  Alcohol12PlusYr              6834 non-null   float64\n",
      " 35  AlcoholDay                   6834 non-null   float64\n",
      " 36  AlcoholYear                  6834 non-null   float64\n",
      " 37  SmokeNow                     6834 non-null   float64\n",
      " 38  Smoke100                     6834 non-null   float64\n",
      " 39  SmokeAge                     6834 non-null   float64\n",
      " 40  Marijuana                    6834 non-null   float64\n",
      " 41  AgeFirstMarij                6834 non-null   float64\n",
      " 42  RegularMarij                 6834 non-null   float64\n",
      " 43  AgeRegMarij                  6834 non-null   float64\n",
      " 44  HardDrugs                    6834 non-null   float64\n",
      " 45  SexEver                      6834 non-null   float64\n",
      " 46  SexAge                       6834 non-null   float64\n",
      " 47  SexNumPartnLife              6834 non-null   float64\n",
      " 48  SexNumPartYear               6834 non-null   float64\n",
      " 49  SameSex                      6834 non-null   float64\n",
      " 50  Race1_Black                  6834 non-null   float64\n",
      " 51  Race1_Hispanic               6834 non-null   float64\n",
      " 52  Race1_Mexican                6834 non-null   float64\n",
      " 53  Race1_Other                  6834 non-null   float64\n",
      " 54  Race1_White                  6834 non-null   float64\n",
      " 55  Education_8th Grade          6834 non-null   float64\n",
      " 56  Education_9 - 11th Grade     6834 non-null   float64\n",
      " 57  Education_College Grad       6834 non-null   float64\n",
      " 58  Education_High School        6834 non-null   float64\n",
      " 59  Education_No Degree          6834 non-null   float64\n",
      " 60  Education_Some College       6834 non-null   float64\n",
      " 61  Education_Unknown            6834 non-null   float64\n",
      " 62  MaritalStatus_Divorced       6834 non-null   float64\n",
      " 63  MaritalStatus_LivePartner    6834 non-null   float64\n",
      " 64  MaritalStatus_Married        6834 non-null   float64\n",
      " 65  MaritalStatus_NeverMarried   6834 non-null   float64\n",
      " 66  MaritalStatus_Separated      6834 non-null   float64\n",
      " 67  MaritalStatus_Unknown        6834 non-null   float64\n",
      " 68  MaritalStatus_Widowed        6834 non-null   float64\n",
      " 69  HomeOwn_Other                6834 non-null   float64\n",
      " 70  HomeOwn_Own                  6834 non-null   float64\n",
      " 71  HomeOwn_Rent                 6834 non-null   float64\n",
      " 72  Work_Looking                 6834 non-null   float64\n",
      " 73  Work_NotWorking              6834 non-null   float64\n",
      " 74  Work_Unknown                 6834 non-null   float64\n",
      " 75  Work_Working                 6834 non-null   float64\n",
      " 76  HealthGen_Excellent          6834 non-null   float64\n",
      " 77  HealthGen_Fair               6834 non-null   float64\n",
      " 78  HealthGen_Good               6834 non-null   float64\n",
      " 79  HealthGen_Poor               6834 non-null   float64\n",
      " 80  HealthGen_Unknown            6834 non-null   float64\n",
      " 81  HealthGen_Vgood              6834 non-null   float64\n",
      " 82  LittleInterest_Most          6834 non-null   float64\n",
      " 83  LittleInterest_None          6834 non-null   float64\n",
      " 84  LittleInterest_Several       6834 non-null   float64\n",
      " 85  LittleInterest_Unknown       6834 non-null   float64\n",
      " 86  Depressed_Most               6834 non-null   float64\n",
      " 87  Depressed_None               6834 non-null   float64\n",
      " 88  Depressed_Several            6834 non-null   float64\n",
      " 89  Depressed_Unknown            6834 non-null   float64\n",
      " 90  Smoke100n_Non-Smoker         6834 non-null   float64\n",
      " 91  Smoke100n_Smoker             6834 non-null   float64\n",
      " 92  Smoke100n_Unknown            6834 non-null   float64\n",
      " 93  SexOrientation_Bisexual      6834 non-null   float64\n",
      " 94  SexOrientation_Heterosexual  6834 non-null   float64\n",
      " 95  SexOrientation_Homosexual    6834 non-null   float64\n",
      " 96  SexOrientation_Unknown       6834 non-null   float64\n",
      "dtypes: float64(97)\n",
      "memory usage: 5.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Choice\n",
    "\n",
    "The first task is to decide which classifier to use (from the ones that we learned this block), i.e. which one would best suit our task and our data. Note that our data are heavily unbalanced, thus you need to do some exploration on how different classifiers handle inbalances in the data (we will discuss some of these techniques during week 3 lecture).\n",
    "\n",
    "It would be possible to do brute force model comparison here - i.e. tune all models and compare which does best with respect to various benchmarks. However, it is also reasonable to do a first round of model comparison by running models (with out of the box parameter settings) on the training data and eliminating some models which performed very poorly.\n",
    "\n",
    "Let the best model win!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_score(model, x_test, y_test):\n",
    "    overall = 0\n",
    "    class_0 = 0\n",
    "    class_1 = 0\n",
    "    for i in range(100):\n",
    "        sample = np.random.choice(len(x_test), len(x_test))\n",
    "        x_sub_test = x_test[sample]\n",
    "        y_sub_test = y_test[sample]\n",
    "        \n",
    "        overall += model.score(x_sub_test, y_sub_test)\n",
    "        class_0 += model.score(x_sub_test[y_sub_test==0], y_sub_test[y_sub_test==0])\n",
    "        class_1 += model.score(x_sub_test[y_sub_test==1], y_sub_test[y_sub_test==1])\n",
    "\n",
    "    return pd.Series([overall / 100., \n",
    "                      class_0 / 100.,\n",
    "                      class_1 / 100.],\n",
    "                      index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])\n",
    "\n",
    "score = lambda model, x_test, y_test: pd.Series([model.score(x_test, y_test), \n",
    "                                                 model.score(x_test[y_test==0], y_test[y_test==0]),\n",
    "                                                 model.score(x_test[y_test==1], y_test[y_test==1])], \n",
    "                                                index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7421719172165347\n",
      "overall accuracy       0.757114\n",
      "accuracy on class 0    0.755796\n",
      "accuracy on class 1    0.801931\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"1105pt\" height=\"552pt\"\r\n",
       " viewBox=\"0.00 0.00 1104.50 552.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 548)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-548 1100.5,-548 1100.5,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M683,-544C683,-544 537,-544 537,-544 531,-544 525,-538 525,-532 525,-532 525,-473 525,-473 525,-467 531,-461 537,-461 537,-461 683,-461 683,-461 689,-461 695,-467 695,-473 695,-473 695,-532 695,-532 695,-538 689,-544 683,-544\"/>\r\n",
       "<text text-anchor=\"start\" x=\"561.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Pulse  49.947</text>\r\n",
       "<text text-anchor=\"start\" x=\"581\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"558.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6834</text>\r\n",
       "<text text-anchor=\"start\" x=\"533\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [3417.0, 3417.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"578\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = flu</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M582,-417.5C582,-417.5 444,-417.5 444,-417.5 438,-417.5 432,-411.5 432,-405.5 432,-405.5 432,-361.5 432,-361.5 432,-355.5 438,-349.5 444,-349.5 444,-349.5 582,-349.5 582,-349.5 588,-349.5 594,-355.5 594,-361.5 594,-361.5 594,-405.5 594,-405.5 594,-411.5 588,-417.5 582,-417.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"484\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"461.5\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3454</text>\r\n",
       "<text text-anchor=\"start\" x=\"440\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1779.333, 0.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"469\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = no_flu</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M576.346,-460.907C566.832,-449.432 556.474,-436.938 546.943,-425.442\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"549.574,-423.131 540.497,-417.667 544.185,-427.599 549.574,-423.131\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"538.171\" y=\"-438.858\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#98ccf1\" stroke=\"black\" d=\"M789.5,-425C789.5,-425 624.5,-425 624.5,-425 618.5,-425 612.5,-419 612.5,-413 612.5,-413 612.5,-354 612.5,-354 612.5,-348 618.5,-342 624.5,-342 624.5,-342 789.5,-342 789.5,-342 795.5,-342 801.5,-348 801.5,-354 801.5,-354 801.5,-413 801.5,-413 801.5,-419 795.5,-425 789.5,-425\"/>\r\n",
       "<text text-anchor=\"start\" x=\"620.5\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">DaysMentHlthBad  14.319</text>\r\n",
       "<text text-anchor=\"start\" x=\"669.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.438</text>\r\n",
       "<text text-anchor=\"start\" x=\"655.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3380</text>\r\n",
       "<text text-anchor=\"start\" x=\"621.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1637.667, 3417.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"675\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = flu</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M643.654,-460.907C651.177,-451.832 659.229,-442.121 666.982,-432.769\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"669.718,-434.953 673.406,-425.021 664.329,-430.485 669.718,-434.953\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"675.732\" y=\"-446.212\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<path fill=\"#bcdef6\" stroke=\"black\" d=\"M640.5,-306C640.5,-306 485.5,-306 485.5,-306 479.5,-306 473.5,-300 473.5,-294 473.5,-294 473.5,-235 473.5,-235 473.5,-229 479.5,-223 485.5,-223 485.5,-223 640.5,-223 640.5,-223 646.5,-223 652.5,-229 652.5,-235 652.5,-235 652.5,-294 652.5,-294 652.5,-300 646.5,-306 640.5,-306\"/>\r\n",
       "<text text-anchor=\"start\" x=\"528\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Age  43.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"525.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.479</text>\r\n",
       "<text text-anchor=\"start\" x=\"511.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3074</text>\r\n",
       "<text text-anchor=\"start\" x=\"481.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1514.03, 2295.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"531\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = flu</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M657.04,-341.907C645.312,-332.379 632.721,-322.148 620.686,-312.37\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"622.84,-309.61 612.872,-306.021 618.426,-315.043 622.84,-309.61\"/>\r\n",
       "</g>\r\n",
       "<!-- 10 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>10</title>\r\n",
       "<path fill=\"#4fa8e8\" stroke=\"black\" d=\"M929.5,-306C929.5,-306 774.5,-306 774.5,-306 768.5,-306 762.5,-300 762.5,-294 762.5,-294 762.5,-235 762.5,-235 762.5,-229 768.5,-223 774.5,-223 774.5,-223 929.5,-223 929.5,-223 935.5,-223 941.5,-229 941.5,-235 941.5,-235 941.5,-294 941.5,-294 941.5,-300 935.5,-306 929.5,-306\"/>\r\n",
       "<text text-anchor=\"start\" x=\"792.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">DirectChol  2.135</text>\r\n",
       "<text text-anchor=\"start\" x=\"814.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.179</text>\r\n",
       "<text text-anchor=\"start\" x=\"804.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 306</text>\r\n",
       "<text text-anchor=\"start\" x=\"770.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [123.636, 1122.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"820\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = flu</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;10 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>2&#45;&gt;10</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M757.307,-341.907C769.116,-332.379 781.795,-322.148 793.914,-312.37\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"796.197,-315.024 801.782,-306.021 791.802,-309.577 796.197,-315.024\"/>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<path fill=\"#fdf5ef\" stroke=\"black\" d=\"M351.5,-187C351.5,-187 194.5,-187 194.5,-187 188.5,-187 182.5,-181 182.5,-175 182.5,-175 182.5,-116 182.5,-116 182.5,-110 188.5,-104 194.5,-104 194.5,-104 351.5,-104 351.5,-104 357.5,-104 363.5,-110 363.5,-116 363.5,-116 363.5,-175 363.5,-175 363.5,-181 357.5,-187 351.5,-187\"/>\r\n",
       "<text text-anchor=\"start\" x=\"190.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">DaysMentHlthBad  7.559</text>\r\n",
       "<text text-anchor=\"start\" x=\"235.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.499</text>\r\n",
       "<text text-anchor=\"start\" x=\"221.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1961</text>\r\n",
       "<text text-anchor=\"start\" x=\"196\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [982.909, 901.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"229\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = no_flu</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>3&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M473.228,-227.282C441.509,-214.485 405.645,-200.016 373.123,-186.894\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"374.18,-183.547 363.596,-183.051 371.561,-190.038 374.18,-183.547\"/>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\r\n",
       "<path fill=\"#84c2ef\" stroke=\"black\" d=\"M640.5,-187C640.5,-187 485.5,-187 485.5,-187 479.5,-187 473.5,-181 473.5,-175 473.5,-175 473.5,-116 473.5,-116 473.5,-110 479.5,-104 485.5,-104 485.5,-104 640.5,-104 640.5,-104 646.5,-104 652.5,-110 652.5,-116 652.5,-116 652.5,-175 652.5,-175 652.5,-181 646.5,-187 640.5,-187\"/>\r\n",
       "<text text-anchor=\"start\" x=\"510\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">PhysActive  0.5</text>\r\n",
       "<text text-anchor=\"start\" x=\"534\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.4</text>\r\n",
       "<text text-anchor=\"start\" x=\"511.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1113</text>\r\n",
       "<text text-anchor=\"start\" x=\"481.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [531.121, 1394.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"531\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = flu</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;7 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>3&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M563,-222.907C563,-214.649 563,-205.864 563,-197.302\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"566.5,-197.021 563,-187.021 559.5,-197.021 566.5,-197.021\"/>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\r\n",
       "<path fill=\"#f7d8c1\" stroke=\"black\" d=\"M158,-68C158,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,-0 12,-0 12,-0 158,-0 158,-0 164,-0 170,-6 170,-12 170,-12 170,-56 170,-56 170,-62 164,-68 158,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"47.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.483</text>\r\n",
       "<text text-anchor=\"start\" x=\"33.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1713</text>\r\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [864.424, 595.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"41\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = no_flu</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;5 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>4&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M203.263,-103.882C186,-93.8269 167.564,-83.0892 150.51,-73.1563\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"152.135,-70.052 141.732,-68.0433 148.611,-76.1008 152.135,-70.052\"/>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\r\n",
       "<path fill=\"#86c3ef\" stroke=\"black\" d=\"M346,-68C346,-68 200,-68 200,-68 194,-68 188,-62 188,-56 188,-56 188,-12 188,-12 188,-6 194,-0 200,-0 200,-0 346,-0 346,-0 352,-0 358,-6 358,-12 358,-12 358,-56 358,-56 358,-62 352,-68 346,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"235.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.402</text>\r\n",
       "<text text-anchor=\"start\" x=\"225.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 248</text>\r\n",
       "<text text-anchor=\"start\" x=\"196\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [118.485, 306.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"241\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = flu</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;6 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>4&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M273,-103.726C273,-95.5175 273,-86.8595 273,-78.56\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"276.5,-78.2996 273,-68.2996 269.5,-78.2996 276.5,-78.2996\"/>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\r\n",
       "<path fill=\"#6bb6ec\" stroke=\"black\" d=\"M543.5,-68C543.5,-68 388.5,-68 388.5,-68 382.5,-68 376.5,-62 376.5,-56 376.5,-56 376.5,-12 376.5,-12 376.5,-6 382.5,-0 388.5,-0 388.5,-0 543.5,-0 543.5,-0 549.5,-0 555.5,-6 555.5,-12 555.5,-12 555.5,-56 555.5,-56 555.5,-62 549.5,-68 543.5,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"428.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.322</text>\r\n",
       "<text text-anchor=\"start\" x=\"418.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 587</text>\r\n",
       "<text text-anchor=\"start\" x=\"384.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [269.939, 1071.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"434\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = flu</text>\r\n",
       "</g>\r\n",
       "<!-- 7&#45;&gt;8 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>7&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M526.881,-103.726C518.801,-94.6054 510.23,-84.93 502.149,-75.8078\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"504.749,-73.4642 495.498,-68.2996 499.509,-78.1058 504.749,-73.4642\"/>\r\n",
       "</g>\r\n",
       "<!-- 9 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>9</title>\r\n",
       "<path fill=\"#d9ecfa\" stroke=\"black\" d=\"M732,-68C732,-68 586,-68 586,-68 580,-68 574,-62 574,-56 574,-56 574,-12 574,-12 574,-6 580,-0 586,-0 586,-0 732,-0 732,-0 738,-0 744,-6 744,-12 744,-12 744,-56 744,-56 744,-62 738,-68 732,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"621.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.494</text>\r\n",
       "<text text-anchor=\"start\" x=\"611.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 526</text>\r\n",
       "<text text-anchor=\"start\" x=\"582\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [261.182, 323.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"627\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = flu</text>\r\n",
       "</g>\r\n",
       "<!-- 7&#45;&gt;9 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>7&#45;&gt;9</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M598.747,-103.726C606.663,-94.6966 615.056,-85.1235 622.983,-76.0816\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"625.845,-78.1263 629.806,-68.2996 620.582,-73.5117 625.845,-78.1263\"/>\r\n",
       "</g>\r\n",
       "<!-- 11 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\r\n",
       "<path fill=\"#4ea7e8\" stroke=\"black\" d=\"M929.5,-187C929.5,-187 774.5,-187 774.5,-187 768.5,-187 762.5,-181 762.5,-175 762.5,-175 762.5,-116 762.5,-116 762.5,-110 768.5,-104 774.5,-104 774.5,-104 929.5,-104 929.5,-104 935.5,-104 941.5,-110 941.5,-116 941.5,-116 941.5,-175 941.5,-175 941.5,-181 935.5,-187 929.5,-187\"/>\r\n",
       "<text text-anchor=\"start\" x=\"779.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Testosterone  579.87</text>\r\n",
       "<text text-anchor=\"start\" x=\"814.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.171</text>\r\n",
       "<text text-anchor=\"start\" x=\"804.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 293</text>\r\n",
       "<text text-anchor=\"start\" x=\"770.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [116.939, 1122.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"820\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = flu</text>\r\n",
       "</g>\r\n",
       "<!-- 10&#45;&gt;11 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>10&#45;&gt;11</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M852,-222.907C852,-214.649 852,-205.864 852,-197.302\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"855.5,-197.021 852,-187.021 848.5,-197.021 855.5,-197.021\"/>\r\n",
       "</g>\r\n",
       "<!-- 14 -->\r\n",
       "<g id=\"node15\" class=\"node\"><title>14</title>\r\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M1084.5,-179.5C1084.5,-179.5 971.5,-179.5 971.5,-179.5 965.5,-179.5 959.5,-173.5 959.5,-167.5 959.5,-167.5 959.5,-123.5 959.5,-123.5 959.5,-117.5 965.5,-111.5 971.5,-111.5 971.5,-111.5 1084.5,-111.5 1084.5,-111.5 1090.5,-111.5 1096.5,-117.5 1096.5,-123.5 1096.5,-123.5 1096.5,-167.5 1096.5,-167.5 1096.5,-173.5 1090.5,-179.5 1084.5,-179.5\"/>\r\n",
       "<text text-anchor=\"start\" x=\"996.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = &#45;0.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"984.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 13</text>\r\n",
       "<text text-anchor=\"start\" x=\"967.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [6.697, 0.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"984\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = no_flu</text>\r\n",
       "</g>\r\n",
       "<!-- 10&#45;&gt;14 -->\r\n",
       "<g id=\"edge14\" class=\"edge\"><title>10&#45;&gt;14</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M913.062,-222.907C931.321,-210.769 951.292,-197.493 969.39,-185.462\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"971.718,-188.118 978.108,-179.667 967.842,-182.288 971.718,-188.118\"/>\r\n",
       "</g>\r\n",
       "<!-- 12 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\r\n",
       "<path fill=\"#4ca7e8\" stroke=\"black\" d=\"M929.5,-68C929.5,-68 774.5,-68 774.5,-68 768.5,-68 762.5,-62 762.5,-56 762.5,-56 762.5,-12 762.5,-12 762.5,-6 768.5,-0 774.5,-0 774.5,-0 929.5,-0 929.5,-0 935.5,-0 941.5,-6 941.5,-12 941.5,-12 941.5,-56 941.5,-56 941.5,-62 935.5,-68 929.5,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"814.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.163</text>\r\n",
       "<text text-anchor=\"start\" x=\"804.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 280</text>\r\n",
       "<text text-anchor=\"start\" x=\"770.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [110.242, 1122.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"820\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = flu</text>\r\n",
       "</g>\r\n",
       "<!-- 11&#45;&gt;12 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>11&#45;&gt;12</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M852,-103.726C852,-95.5175 852,-86.8595 852,-78.56\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"855.5,-78.2996 852,-68.2996 848.5,-78.2996 855.5,-78.2996\"/>\r\n",
       "</g>\r\n",
       "<!-- 13 -->\r\n",
       "<g id=\"node14\" class=\"node\"><title>13</title>\r\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M1084.5,-68C1084.5,-68 971.5,-68 971.5,-68 965.5,-68 959.5,-62 959.5,-56 959.5,-56 959.5,-12 959.5,-12 959.5,-6 965.5,-0 971.5,-0 971.5,-0 1084.5,-0 1084.5,-0 1090.5,-0 1096.5,-6 1096.5,-12 1096.5,-12 1096.5,-56 1096.5,-56 1096.5,-62 1090.5,-68 1084.5,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"996.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = &#45;0.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"984.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 13</text>\r\n",
       "<text text-anchor=\"start\" x=\"967.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [6.697, 0.0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"984\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = no_flu</text>\r\n",
       "</g>\r\n",
       "<!-- 11&#45;&gt;13 -->\r\n",
       "<g id=\"edge13\" class=\"edge\"><title>11&#45;&gt;13</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M917.536,-103.726C933.451,-93.8245 950.417,-83.269 966.155,-73.4774\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"968.116,-76.3797 974.758,-68.1252 964.418,-70.4361 968.116,-76.3797\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x229a19f59e8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### fancy models that solve the problem\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn.tree as tree\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0, class_weight='balanced', max_depth=4)\n",
    "print(np.mean(cross_val_score(clf, X_train, y_train, cv=5, scoring='balanced_accuracy')))\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(expected_score(clf, X_test.to_numpy(), y_test.to_numpy()))\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                      feature_names=X_train.columns.to_list(),  \n",
    "                      class_names=['no_flu', 'flu'],  \n",
    "                      filled=True, rounded=True,\n",
    "                      special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On evaluation\n",
    "\n",
    "### AUROC\n",
    "\n",
    "As mentioned abbove, we will use the accuracy scores for each class and for the whole dataset, as well as the AUROC score from Kaggle platform. You can coimpute AUROC locally (e.g. on your train/validation set) by calling the relevant scikit learn function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###AUROC locally\n",
    "\n",
    "#score = roc_auc_score(real_labels, predicted_labels)\n",
    "\n",
    "#real_labels: the ground truth (0 or 1)\n",
    "#predicted_labels: labels predicted by your algorithm (0 or 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy (per class)\n",
    "\n",
    "Below there is a function that will be handy for your models. It computes the accuracy per-class, based on a model you pass as parameter and a dataset (split to x/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extended_score(model, x_test, y_test):\n",
    "    overall = 0\n",
    "    class_0 = 0\n",
    "    class_1 = 0\n",
    "    for i in range(100):\n",
    "        sample = np.random.choice(len(x_test), len(x_test))\n",
    "        x_sub_test = x_test[sample]\n",
    "        y_sub_test = y_test[sample]\n",
    "        \n",
    "        overall += model.score(x_sub_test, y_sub_test)\n",
    "        class_0 += model.score(x_sub_test[y_sub_test==0], y_sub_test[y_sub_test==0])\n",
    "        class_1 += model.score(x_sub_test[y_sub_test==1], y_sub_test[y_sub_test==1])\n",
    "\n",
    "    return pd.Series([overall / 100., \n",
    "                      class_0 / 100.,\n",
    "                      class_1 / 100.],\n",
    "                      index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same job as before, but faster?\n",
    "\n",
    "score = lambda model, x_val, y_val: pd.Series([model.score(x_val, y_val), \n",
    "                                                 model.score(x_val[y_val==0], y_val[y_val==0]),\n",
    "                                                 model.score(x_val[y_val==1], y_val[y_val==1])], \n",
    "                                                index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution extraction for Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you extract your solutions (predictions) in the correct format required by Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Conclusions\n",
    "\n",
    "Highlight at the end of your notebook, which were the top-3 approaches that produced the best scores for you. That is, provide a table with the scores you got (on the AUROC score you get from Kaggle) and make sure that you judge these in relation to your work on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
